import torch
from diffusers
import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils
import export_to_video
from huggingface_hub
import HfFolder
import shutil
import os
from IPython.display
import Video

# Set your Hugging Face token
HfFolder.save_token("sDg06pJA57ZfkA6BFgayTLRZuMcRQ1IUKzVwyqLl")# Replace with your Hugging Face token

# Load the model
pipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype = torch.float16, variant = "fp16")
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()

# Function to generate video from text prompt
def generate_video(prompt): #Generate the video frames
video_frames = pipe(prompt, num_inference_steps = 25).frames

# Reshape or select the appropriate portion of the video_frames array
video_frames_reshaped = [frame
    for frame in video_frames[0]
]

# Export the video frames to a video
video_path = export_to_video(video_frames_reshaped)

# Define the location to save the video
output_video_path = "generated_video.mp4"

#
Move or rename the generated video file to the desired location
shutil.move(video_path, output_video_path)

# Return the path to the generated video
return output_video_path

# Input text prompt
prompt = "Two romantic boy and girl dancing"#
You can change this

# Generate the video
if prompt:
    print("Generating video... Please wait.")
video_path = generate_video(prompt)

# Check
if the video file was generated
if os.path.exists(video_path):
    print(f "Video saved as: {video_path}")

# Display video in Colab
Video(video_path)# This will display the video in the notebook

# Provide a download link
from google.colab
import files
files.download(video_path)# This will trigger the file download
else :
    print("Error: Video generation failed.")
else :
    print("Please enter a prompt.")